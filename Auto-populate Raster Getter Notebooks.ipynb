{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert cells programmaticaly to python notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob(\"*.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [fn for fn in glob('*') \n",
    "         if not os.path.basename(fn).endswith('done.ipynb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cli.027.ipynb',\n",
       " 'dis.004.ipynb',\n",
       " 'dis.005.ipynb',\n",
       " 'dis.007.ipynb',\n",
       " 'dis.008.ipynb',\n",
       " 'dis.012.ipynb',\n",
       " 'ene.003.1.ipynb',\n",
       " 'ene.003.2.ipynb',\n",
       " 'ene.004.ipynb',\n",
       " 'ene.006.ipynb',\n",
       " 'ene.015.ipynb',\n",
       " 'ene.018.ipynb',\n",
       " 'foo.005.ipynb',\n",
       " 'foo.017.ipynb',\n",
       " 'foo.023.ipynb',\n",
       " 'foo.028.ipynb',\n",
       " 'soc.031.ipynb',\n",
       " 'soc.032.ipynb',\n",
       " 'wat.015.ipynb',\n",
       " 'wat.029.ipynb']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_overwrite = files[6:]\n",
    "files_to_overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exemplar = files_to_overwrite[0]\n",
    "f = open(exemplar, 'r')\n",
    "exemplar = json.load(f)\n",
    "\n",
    "for cell in exemplar[\"cells\"]:\n",
    "    if 'outputs' in cell.keys():\n",
    "        cell['outputs'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Historical Ozone Concentrations\\tcli.027\\thttps://explorer.earthengine.google.com/#detail/TOMS%2FMERGED']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['import numpy as np\\n',\n",
       "   'import pandas as pd\\n',\n",
       "   'import rasterio\\n',\n",
       "   '\\n',\n",
       "   'import boto3\\n',\n",
       "   'import requests as req\\n',\n",
       "   '\\n',\n",
       "   'from matplotlib import pyplot as plt\\n',\n",
       "   '%matplotlib inline\\n',\n",
       "   'import os\\n',\n",
       "   'import sys\\n',\n",
       "   'import threading']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Establish s3 location']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['s3_bucket = \"wri-public-data\"\\n',\n",
       "   's3_folder = \"resoucewatch/<tif_folder_name>\"\\n',\n",
       "   's3_file = \"<tif_file_name[s]>.tif\"\\n',\n",
       "   '\\n',\n",
       "   's3_key_orig = s3_folder + s3_file\\n',\n",
       "   's3_key_edit = s3_key_orig[0:-4] + \"_edit.tif\"']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Create local staging folder for holding data']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['!mkdir staging\\n',\n",
       "   'os.chdir(\"staging\")\\n',\n",
       "   'staging_folder = os.getcwd()\\nos.environ[\"Z_STAGING_FOLDER\"] = staging_folder']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['If data already on s3, create a staging key and download to staging folder']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['staging_file = \"/<staging_tif_name>.tif\"\\n',\n",
       "   'staging_key_orig = staging_folder + staging_file\\n',\n",
       "   'staging_key_edit = staging_key_orig[0:-4] + \"_edit.tif\"\\n\\n',\n",
       "   's3 = boto3.resource(\"s3\")\\n',\n",
       "   's3.meta.client.download_file(s3_bucket, s3_key_orig, staging_key_orig)\\n',\n",
       "   's3.meta.client.download_file(s3_bucket, s3_key_edit, staging_key_edit)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['If data in local storage, move to staging folder']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['local_folder = \"/Users/nathansuberi/Desktop/WRI_Programming/RW_Data\"\\n',\n",
       "   'rw_data_type = \"/<data_topic>\"\\n',\n",
       "   '# Topics include: [Society, Food, Forests, Water, Energy, Climate, Cities, Biodiversity, Commerce, Disasters]\\n',\n",
       "   'local_file = \"/<file_name>.tif\"\\n',\n",
       "   'local_key = local_folder + rw_data_type + local_file\\n',\n",
       "   '\\n',\n",
       "   'staging_key_orig = staging_folder + local_file\\n',\n",
       "   'staging_key_edit = staging_key_orig[0:-4] + \"_edit.tif\"\\n',\n",
       "   '\\n',\n",
       "   'os.rename(local_key, staging_key_orig)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['<b>Regardless of any needed edits, upload original file</b>\\n',\n",
       "   '\\n',\n",
       "   '<i>Upload tif to S3 folder</i>\\n',\n",
       "   '\\n',\n",
       "   'http://boto3.readthedocs.io/en/latest/guide/s3-example-creating-buckets.html\\n',\n",
       "   '\\n',\n",
       "   '<i>Monitor Progress of Upload</i>\\n',\n",
       "   '\\n',\n",
       "   'http://boto3.readthedocs.io/en/latest/_modules/boto3/s3/transfer.html\\n',\n",
       "   'https://boto3.readthedocs.io/en/latest/guide/s3.html#using-the-transfer-manager']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['s3 = boto3.client(\"s3\")\\n',\n",
       "   '\\n',\n",
       "   'class ProgressPercentage(object):\\n',\n",
       "   '        def __init__(self, filename):\\n',\n",
       "   '            self._filename = filename\\n',\n",
       "   '            self._size = float(os.path.getsize(filename))\\n',\n",
       "   '            self._seen_so_far = 0\\n',\n",
       "   '            self._lock = threading.Lock()\\n',\n",
       "   '\\n',\n",
       "   '        def __call__(self, bytes_amount):\\n',\n",
       "   \"            # To simplify we'll assume this is hooked up\\n\",\n",
       "   '            # to a single filename.\\n',\n",
       "   '            with self._lock:\\n',\n",
       "   '                self._seen_so_far += bytes_amount\\n',\n",
       "   '                percentage = (self._seen_so_far / self._size) * 100\\n',\n",
       "   '                sys.stdout.write(\\n',\n",
       "   '                    \"\\\\r%s  %s / %s  (%.2f%%)\" % (\\n',\n",
       "   '                        self._filename, self._seen_so_far, self._size,\\n',\n",
       "   '                        percentage))\\n',\n",
       "   '                sys.stdout.flush()']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Defined above:\\n',\n",
       "   '# s3_bucket\\n',\n",
       "   '# s3_key_orig\\n',\n",
       "   '# s3_key_edit\\n',\n",
       "   '# staging_key_orig\\n',\n",
       "   '# staging_key_edit\\n',\n",
       "   '\\n',\n",
       "   's3.upload_file(staging_key_orig, s3_bucket, s3_key_orig,\\n',\n",
       "   '                         Callback=ProgressPercentage(staging_key_orig))']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Check for compression, projection\\n',\n",
       "   '\\n',\n",
       "   'Create edit file if necessary']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Check Compression, Projection\\n',\n",
       "   'os.environ[\"Z_FILE_LOC\"] = staging_key_orig\\n',\n",
       "   '\\n',\n",
       "   '# Use GDAL command line tools... \\n',\n",
       "   '# can replace with rasterio eventually\\n',\n",
       "   '!gdalinfo $Z_FILE_LOC']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Project & Compress\\n',\n",
       "   '\\n',\n",
       "   '# Project with gdalwarp, compress with gdal_translate\\n',\n",
       "   '# https://gis.stackexchange.com/questions/89444/file-size-inflation-normal-with-gdalwarp\\n',\n",
       "   '# http://www.gdal.org/gdalwarp.html\\n',\n",
       "   '# http://www.perrygeo.com/lazy-raster-processing-with-gdal-vrts.html\\n',\n",
       "   '\\n',\n",
       "   'staging_temp_file = staging_folder + \"/temp.vrt\"\\n',\n",
       "   'os.environ[\"Z_FILE_TEMP\"] = staging_temp_file\\n',\n",
       "   '!gdalwarp -t_srs EPSG:4326 -of vrt $ZFILELOC $Z_FILE_TEMP\\n',\n",
       "   '\\n',\n",
       "   '# Compress\\n',\n",
       "   '# http://www.gdal.org/gdal_translate.html\\n',\n",
       "   '\\n',\n",
       "   'os.environ[\"Z_FILE_DEST\"] = staging_key_edit\\n',\n",
       "   '!gdal_translate -of GTiff -co COMPRESS=LZW $Z_FILE_TEMP $Z_FILE_DEST']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False, 'scrolled': True},\n",
       "  'outputs': [],\n",
       "  'source': ['!gdalinfo $Z_FILE_DEST']},\n",
       " {'cell_type': 'markdown', 'metadata': {}, 'source': ['Examine data\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# with rasterio.open(staging_key_orig) as src:\\nwith rasterio.open(staging_key_edit) as src:\\n',\n",
       "   '    profile = src.profile\\n',\n",
       "   '    print(profile)\\n',\n",
       "   '    data = src.read(1)']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['data']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['# This works if all the raster values are positive whole numbers\\n',\n",
       "   'counts = {}\\n',\n",
       "   'for row in range(data[:,0].shape[0]):\\n',\n",
       "   '    cts = np.bincount(data[row,:])\\n',\n",
       "   '    for val, ct in enumerate(cts):\\n',\n",
       "   '        try:\\n',\n",
       "   '            counts[val] += ct\\n',\n",
       "   '        except:\\n',\n",
       "   '            counts[val] = ct']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['counts']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Upload edited files to S3']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Defined above:\\n',\n",
       "   '# s3_bucket\\n',\n",
       "   '# s3_key_orig\\n',\n",
       "   '# s3_key_edit\\n',\n",
       "   '# staging_key_orig\\n',\n",
       "   '# staging_key_edit\\n',\n",
       "   '\\n',\n",
       "   's3.upload_file(staging_key_edit, s3_bucket, s3_key_edit,\\n',\n",
       "   '                         Callback=ProgressPercentage(staging_key_edit))']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Layer definition\\n',\n",
       "   '\\n',\n",
       "   'https://github.com/resource-watch/notebooks/blob/master/ResourceWatch/Api_definition/layer_definition.ipynb']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': []},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Upload to server destination']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['# Too big for ArcGIS Online to upload using their web interface... 1 GB limit']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Remove data from computer / instance']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': None,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['os.chdir(\"..\")\\n', '!rm -r $Z_STAGING_FOLDER']}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplar[\"cells\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add extends\n",
    "# -te -180 # # # \n",
    "# Add compression to gdalwarp command\n",
    "# Mollweide doesn't have an EPSG code, huh\n",
    "# adf\n",
    "# adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standard_fare = [\n",
    "    \n",
    "{'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': [\n",
    "  'Import libraries']},\n",
    "{'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': True},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "\"\"\"# Libraries for downloading data from remote server (may be ftp)\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from contextlib import closing\n",
    "import shutil\n",
    "\n",
    "# Library for uploading/downloading data to/from S3\n",
    "import boto3\n",
    "\n",
    "# Libraries for handling data\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "# from netCDF4 import Dataset\n",
    "# import pandas as pd\n",
    "# import scipy\n",
    "\n",
    "# Libraries for various helper functions\n",
    "# from datetime import datetime\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "from glob import glob\"\"\"]},\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "{'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': [\n",
    "  's3 tools']},\n",
    "{'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': False},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "  \n",
    "\"\"\"s3_upload = boto3.client(\"s3\")\n",
    "s3_download = boto3.resource(\"s3\")\n",
    "\n",
    "s3_bucket = \"wri-public-data\"\n",
    "s3_folder = \"resourcewatch/raster/*\"\n",
    "\n",
    "s3_file = \"*.tif\"\n",
    "\n",
    "s3_key_orig = s3_folder + s3_file\n",
    "s3_key_edit = s3_key_orig[0:-4] + \"_edit.tif\"\n",
    "\n",
    "\n",
    "class ProgressPercentage(object):\n",
    "        def __init__(self, filename):\n",
    "            self._filename = filename\n",
    "            self._size = float(os.path.getsize(filename))\n",
    "            self._seen_so_far = 0\n",
    "            self._lock = threading.Lock()\n",
    "\n",
    "        def __call__(self, bytes_amount):\n",
    "            # To simplify we'll assume this is hooked up\n",
    "            # to a single filename.\n",
    "            with self._lock:\n",
    "                self._seen_so_far += bytes_amount\n",
    "                percentage = (self._seen_so_far / self._size) * 100\n",
    "                sys.stdout.write(\"\\\\r%s  %s / %s  (%.2f%%)\"%(\n",
    "                        self._filename, self._seen_so_far, self._size,\n",
    "                        percentage))\n",
    "                sys.stdout.flush()\"\"\" ]},\n",
    "   \n",
    "    \n",
    "    \n",
    "{'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': [\n",
    "  'Potentially useful for online directories']},   \n",
    "    \n",
    "    {'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': False},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "\"\"\"# View files in a ftp, see cli.015\n",
    "remote_path = \"ftp://cidportal.jrc.ec.europa.eu/jrc-opendata/EDGAR/datasets/v431_v2\"\n",
    "remote_path_BC = remote_path + \"/BC/TOTALS/\"\n",
    "file = req.urlopen(remote_path_BC).read().splitlines()\n",
    "\n",
    "# Copy from https or ftp\n",
    "with(closing(urlopen(online_folder + most_recent))) as r:\n",
    "    with(open(local_orig, 'wb')) as f:\n",
    "        shutil.copyfileobj(r, f)\"\"\"     \n",
    "  ]},\n",
    "    \n",
    "    \n",
    "{'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': [\n",
    "  'Define local file locations']},             \n",
    "{'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': False},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "\"\"\"local_folder = \"/Users/nathansuberi/Desktop/RW_Data/Rasters/*\"\n",
    "file_name = \"*\"]\n",
    "\n",
    "local_orig = local_folder + file_name\n",
    "\n",
    "orig_extension_length = 4 #4 for each char in .tif\n",
    "local_edit = local_orig[:-orig_extension_length] + \"edit.tif\" \"\"\"     \n",
    "  ]},\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "{'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': ['Use rasterio to reproject and compress']},\n",
    "{'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': False},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "\n",
    "\"\"\"# Note - this is the core of Vizz's netcdf2tif function\n",
    "\n",
    "with rio.open(local_orig, 'r') as src:\n",
    "    # This assumes data is readable by rasterio\n",
    "    # May need to open instead with netcdf4.Dataset, for example\n",
    "   \n",
    "    data = src.read()[0]\n",
    "    \n",
    "    rows = data.shape[0]\n",
    "    columns = data.shape[1]\n",
    "    \n",
    "    print(rows)\n",
    "    print(columns)\n",
    "    \n",
    "    # Latitude bounds\n",
    "    south_lat = -90\n",
    "    north_lat = 90\n",
    "\n",
    "    # Longitude bounds\n",
    "    west_lon = -180\n",
    "    east_lon = 180\n",
    "    \n",
    "    transform = rasterio.transform.from_bounds(west_lon, south_lat, east_lon, north_lat, columns, rows)\n",
    "    \n",
    "    # Profile\n",
    "    \n",
    "    no_data_val = *\n",
    "    target_projection = 'EPSG:4326'\n",
    "    target_data_type = np.float64\n",
    "    \n",
    "    profile = {\n",
    "        'driver':'GTiff', \n",
    "        'height':rows, \n",
    "        'width':columns, \n",
    "        'count':1, \n",
    "        'dtype':target_data_type, \n",
    "        'crs':target_projection, \n",
    "        'transform':transform, \n",
    "        'compress':'lzw', \n",
    "        'nodata': no_data_val\n",
    "    }\n",
    "    \n",
    "    with rio.open(local_edit, \"w\", **profile) as dst:\n",
    "        dst.write(data.astype(profile[\"dtype\"]), 1)\"\"\"\n",
    "  ]},\n",
    "\n",
    "    \n",
    "\n",
    " {'cell_type': 'markdown',\n",
    "  'metadata': {},\n",
    "  'source': ['Upload orig and edit files to s3']},\n",
    " {'cell_type': 'code',\n",
    "  'execution_count': 0,\n",
    "  'metadata': {'collapsed': False},\n",
    "  'outputs': [],\n",
    "  'source': [\n",
    "      \n",
    "\"\"\"# Original\n",
    "s3_upload.upload_file(local_orig, s3_bucket, s3_key_orig,\n",
    "                         Callback=ProgressPercentage(local_orig))\n",
    "\n",
    "# Edit\n",
    "s3_upload.upload_file(local_edit, s3_bucket, s3_key_edit,\n",
    "                         Callback=ProgressPercentage(local_edit))\"\"\"\n",
    "      \n",
    "  ]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_fare = standard_fare\n",
    "for cell in s_fare:\n",
    "    if 'execution_count' in cell.keys():\n",
    "        cell['execution_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_type': 'markdown', 'metadata': {}, 'source': ['Import libraries']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': True},\n",
       "  'outputs': [],\n",
       "  'source': ['# Libraries for downloading data from remote server (may be ftp)\\nimport requests\\nfrom urllib.request import urlopen\\nfrom contextlib import closing\\nimport shutil\\n\\n# Library for uploading/downloading data to/from S3\\nimport boto3\\n\\n# Libraries for handling data\\nimport rasterio as rio\\nimport numpy as np\\n# from netCDF4 import Dataset\\n# import pandas as pd\\n# import scipy\\n\\n# Libraries for various helper functions\\n# from datetime import datetime\\nimport os\\nimport threading\\nimport sys\\nfrom glob import glob']},\n",
       " {'cell_type': 'markdown', 'metadata': {}, 'source': ['s3 tools']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['s3_upload = boto3.client(\"s3\")\\ns3_download = boto3.resource(\"s3\")\\n\\ns3_bucket = \"wri-public-data\"\\ns3_folder = \"resourcewatch/raster/*\"\\n\\ns3_file = \"*.tif\"\\n\\ns3_key_orig = s3_folder + s3_file\\ns3_key_edit = s3_key_orig[0:-4] + \"_edit.tif\"\\n\\n\\nclass ProgressPercentage(object):\\n        def __init__(self, filename):\\n            self._filename = filename\\n            self._size = float(os.path.getsize(filename))\\n            self._seen_so_far = 0\\n            self._lock = threading.Lock()\\n\\n        def __call__(self, bytes_amount):\\n            # To simplify we\\'ll assume this is hooked up\\n            # to a single filename.\\n            with self._lock:\\n                self._seen_so_far += bytes_amount\\n                percentage = (self._seen_so_far / self._size) * 100\\n                sys.stdout.write(\"\\\\r%s  %s / %s  (%.2f%%)\"%(\\n                        self._filename, self._seen_so_far, self._size,\\n                        percentage))\\n                sys.stdout.flush()']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Potentially useful for online directories']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# View files in a ftp, see cli.015\\nremote_path = \"ftp://cidportal.jrc.ec.europa.eu/jrc-opendata/EDGAR/datasets/v431_v2\"\\nremote_path_BC = remote_path + \"/BC/TOTALS/\"\\nfile = req.urlopen(remote_path_BC).read().splitlines()\\n\\n# Copy from https or ftp\\nwith(closing(urlopen(online_folder + most_recent))) as r:\\n    with(open(local_orig, \\'wb\\')) as f:\\n        shutil.copyfileobj(r, f)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Define local file locations']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['local_folder = \"/Users/nathansuberi/Desktop/RW_Data/Rasters/*\"\\nfile_name = \"*\"]\\n\\nlocal_orig = local_folder + file_name\\n\\norig_extension_length = 4 #4 for each char in .tif\\nlocal_edit = local_orig[:-orig_extension_length] + \"edit.tif\" ']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Use rasterio to reproject and compress']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Note - this is the core of Vizz\\'s netcdf2tif function\\n\\nwith rio.open(local_orig, \\'r\\') as src:\\n   \\n   # This assumes data is readable by rasterio\\n   # May need to open instead with netcdf4.Dataset, for example\\n   \\n    data = src.read()[0]\\n    \\n    rows = data.shape[0]\\n    columns = data.shape[1]\\n    \\n    print(rows)\\n    print(columns)\\n    \\n    # Latitude bounds\\n    south_lat = -90\\n    north_lat = 90\\n\\n    # Longitude bounds\\n    west_lon = -180\\n    east_lon = 180\\n    \\n    transform = rasterio.transform.from_bounds(west_lon, south_lat, east_lon, north_lat, columns, rows)\\n    \\n    # Profile\\n    \\n    no_data_val = *\\n    target_projection = \\'EPSG:4326\\'\\n    target_data_type = np.float64\\n    \\n    profile = {\\n        \\'driver\\':\\'GTiff\\', \\n        \\'height\\':rows, \\n        \\'width\\':columns, \\n        \\'count\\':1, \\n        \\'dtype\\':target_data_type, \\n        \\'crs\\':target_projection, \\n        \\'transform\\':transform, \\n        \\'compress\\':\\'lzw\\', \\n        \\'nodata\\': no_data_val\\n    }\\n    \\n    with rio.open(local_edit, \"w\", **profile) as dst:\\n        dst.write(data.astype(profile[\"dtype\"]), 1)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Upload orig and edit files to s3']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 0,\n",
       "  'metadata': {'collapsed': False},\n",
       "  'outputs': [],\n",
       "  'source': ['# Original\\ns3_upload.upload_file(local_orig, s3_bucket, s3_key_orig,\\n                         Callback=ProgressPercentage(local_orig))\\n\\n# Edit\\ns3_upload.upload_file(local_edit, s3_bucket, s3_key_edit,\\n                         Callback=ProgressPercentage(local_edit))']}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(files_to_overwrite[3], 'r')\n",
    "j = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j[\"cells\"] = [j[\"cells\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fare in standard_fare:\n",
    "    j[\"cells\"].append(fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test.ipynb', 'w') as outfile:\n",
    "    json.dump(j, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cli.027.ipynb',\n",
       " 'dis.004.ipynb',\n",
       " 'dis.005.ipynb',\n",
       " 'dis.007.ipynb',\n",
       " 'dis.008.ipynb',\n",
       " 'dis.012.ipynb',\n",
       " 'ene.003.1.ipynb',\n",
       " 'ene.003.2.ipynb',\n",
       " 'ene.004.ipynb',\n",
       " 'ene.006.ipynb',\n",
       " 'ene.015.ipynb',\n",
       " 'ene.018.ipynb',\n",
       " 'foo.005.ipynb',\n",
       " 'foo.017.ipynb',\n",
       " 'foo.023.ipynb',\n",
       " 'foo.028.ipynb',\n",
       " 'soc.031.ipynb',\n",
       " 'soc.032.ipynb',\n",
       " 'wat.015.ipynb',\n",
       " 'wat.029.ipynb']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cli.027.ipynb\n",
      "dis.004.ipynb\n",
      "dis.005.ipynb\n",
      "dis.007.ipynb\n",
      "dis.008.ipynb\n",
      "dis.012.ipynb\n",
      "ene.003.1.ipynb\n",
      "ene.003.2.ipynb\n",
      "ene.004.ipynb\n",
      "ene.006.ipynb\n",
      "ene.015.ipynb\n",
      "ene.018.ipynb\n",
      "foo.005.ipynb\n",
      "foo.017.ipynb\n",
      "foo.023.ipynb\n",
      "foo.028.ipynb\n",
      "soc.031.ipynb\n",
      "soc.032.ipynb\n",
      "wat.015.ipynb\n",
      "wat.029.ipynb\n"
     ]
    }
   ],
   "source": [
    "for file in files_to_overwrite:\n",
    "    print(file)\n",
    "    f = open(file, 'r')\n",
    "    j = json.load(f)\n",
    "    j[\"cells\"] = [j[\"cells\"][0]]\n",
    "    for fare in s_fare:\n",
    "        j[\"cells\"].append(fare)\n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(j, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
