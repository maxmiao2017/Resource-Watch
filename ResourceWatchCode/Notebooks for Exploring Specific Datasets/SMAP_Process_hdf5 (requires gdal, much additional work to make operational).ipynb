{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import urllib\n",
    "import ftplib\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from osgeo import gdal_array\n",
    "from osgeo import osr\n",
    "\n",
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avail_files = [\"SMAP_L4_SM_gph_20170806T013000_Vv3030_001.h5\"]\n",
    "bucket = \"resourcewatch/raster/foo_001_soil_moisture_active_passive/\"\n",
    "for fileObj in avail_files:\n",
    "    s3.download_file(\"wri-public-data\", bucket + fileObj, fileObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Command to get\n",
    "# wget --load-cookies ./.urs_cookies --save-cookies ./.urs_cookies --keep-session-cookies --no-check-certificate --auth-no-challenge=on -r --reject \"index.html*\" -np -e robots=off https://n5eil01u.ecs.nsidc.org/SMAP/SPL3SMA.003/2015.04.13/SMAP_L3_SM_A_20150413_R13080_002.h5 -P SMAP_L3_SM_A_20150413_R13080_002.h5  \n",
    "\n",
    "# ^ doesn't work... downloading directly does\n",
    "\n",
    "os.listdir()\n",
    "f = h5py.File(\"SMAP_L2_SM_AP_01056_D_20150413T184423_R13080_001.h5\", \"r\")\n",
    "# Unable to open file - corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gdalinfo SMAP_L4_SM_gph_20170806T013000_Vv3030_001.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List data groups\n",
    "print(list(f.items()))\n",
    "\n",
    "dataset = f[\"Soil_Moisture_Retrieval_Data_3km\"]\n",
    "\n",
    "# No attributes\n",
    "print(list(dataset.attrs))\n",
    "# List keys\n",
    "print(list(dataset.keys()))\n",
    "\n",
    "# Retrieve dataset\n",
    "data = dataset.get(\"soil_moisture_v_3km\")\n",
    "lats = dataset.get(\"latitude_3km\")\n",
    "lons = dataset.get(\"longitude_3km\")\n",
    "\n",
    "# See attributes of dataset\n",
    "print(list(data.attrs.keys()), \"\\n\")\n",
    "for key in data.attrs.keys():\n",
    "    print(key, \": \",  data.attrs[key])\n",
    "\n",
    "\n",
    "    \n",
    "# Inspect data \n",
    "print('\\n')\n",
    "print(\"data shape: \", data.shape)\n",
    "\n",
    "# Select only valid vals\n",
    "#valid_vals = np.where(data[:]==-9999., False, data[:])\n",
    "invalid_data_ind = np.where(data[:]==-9999.)\n",
    "\n",
    "valid_data_ind = np.where(data[:]==-9999.)\n",
    "\n",
    "print('\\n')\n",
    "valid_data = np.delete(data, invalid_data_ind)\n",
    "valid_lats = np.delete(lats, invalid_data_ind)\n",
    "valid_lons = np.delete(lons, invalid_data_ind)\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HDF5toGeotiff(data, lat, lon, fileLocation, fileName):\n",
    "    parameter = nc_vars[3]\n",
    "    lats = nc_fid.variables['latitude'][:]  # extract/copy the data\n",
    "    lons = nc_fid.variables['longitude'][:]\n",
    "    times = nc_fid.variables['time'][:]\n",
    "    timeUnit = nc_fid.variables[\"time\"].getncattr(\"units\")\n",
    "    timeNormal =[]\n",
    "    for time in times:\n",
    "        if timeUnit == (\"days since 1900-01-01 00:00:00\") or (timeUnit ==\"Days since 1900-01-01\"):\n",
    "            timeNormal.append(datetime.datetime(1900,1,1) + datetime.timedelta(days=time))\n",
    "        elif timeUnit == \"days since 1901-01-01 00:00:00\":\n",
    "            timeNormal.append(datetime.datetime(1901,1,1) + datetime.timedelta(days=time))\n",
    "        else:\n",
    "            print \"Error\"\n",
    "            timeNormal.append(-9999)\n",
    "            \n",
    "    for i in range(0,len(timeNormal)):\n",
    "        #print timeNormal[i].year\n",
    "        Z = nc_fid.variables[parameter][i, :, :]\n",
    "        Z[Z<-9990]= -9999\n",
    "        Z[Z>1e19] = -9999\n",
    "        outputFilename = netCDFInputBaseName + \"I%0.3dY%0.2dM%0.2d.tif\" %(i,timeNormal[i].year,timeNormal[i].month)\n",
    "        writefilename = os.path.join(EC2_OUTPUTPATH,outputFilename)\n",
    "        writeFile(writefilename,geotransform,geoproj,Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeFile(filename,geotransform,geoprojection,data):\n",
    "    (x,y) = data.shape\n",
    "    format = \"GTiff\"\n",
    "    driver = gdal.GetDriverByName(format)\n",
    "    # you can change the dataformat but be sure to be able to store negative values including -9999\n",
    "    dst_datatype = gdal.GDT_Float32\n",
    "    dst_ds = driver.Create(filename,y,x,1,dst_datatype, [ 'COMPRESS=LZW' ])\n",
    "    dst_ds.GetRasterBand(1).SetNoDataValue(-9999)\n",
    "    dst_ds.GetRasterBand(1).WriteArray(data)\n",
    "    dst_ds.SetGeoTransform(geotransform)\n",
    "    dst_ds.SetProjection(geoprojection)\n",
    "    dst_ds = None\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gdal_translate -of GTiff -a_srs '+proj=longlat +datum=WGS84 +no_defs ' 'HDF5:\"SMAP_L2_SM_AP_01056_D_20150413T184423_R13080_001.h5\"://Soil_Moisture_Retrieval_Data_3km' SMAP.tif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "\n",
    "def hdf_subdataset_extraction(hdf_file, dst_dir, subdataset):\n",
    "    \"\"\"unpack a single subdataset from a HDF5 container and write to GeoTiff\"\"\"\n",
    "    # open the dataset\n",
    "    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n",
    "    band_ds = gdal.Open(hdf_ds.GetSubDatasets()[subdataset][0], gdal.GA_ReadOnly)\n",
    "\n",
    "    # read into numpy array\n",
    "    band_array = band_ds.ReadAsArray().astype(np.int16)\n",
    "\n",
    "    # convert no_data values\n",
    "    # band_array[band_array == -28672] = -32768\n",
    "\n",
    "    # build output path\n",
    "    band_path = os.path.join(dst_dir, os.path.basename(os.path.splitext(hdf_file)[0]) + \"-sd\" + str(subdataset+1) + \".tif\")\n",
    "\n",
    "    # write raster\n",
    "    out_ds = gdal.GetDriverByName('GTiff').Create(band_path,\n",
    "                                                  band_ds.RasterXSize,\n",
    "                                                  band_ds.RasterYSize,\n",
    "                                                  1,  #Number of bands\n",
    "                                                  gdal.GDT_Int16,\n",
    "                                                  ['COMPRESS=LZW', 'TILED=YES'])\n",
    "    out_ds.SetGeoTransform(band_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(band_ds.GetProjection())\n",
    "    out_ds.GetRasterBand(1).WriteArray(band_array)\n",
    "    out_ds.GetRasterBand(1).SetNoDataValue(-32768)\n",
    "\n",
    "    out_ds = None  #close dataset to write to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hdf_ds = gdal.Open('HDF5:\"SMAP_L2_SM_AP_01056_D_20150413T184423_R13080_001.h5\"://Soil_Moisture_Retrieval_Data_3km', gdal.GA_ReadOnly)\n",
    "hdf_ds = gdal.Open(\"SMAP_L2_SM_AP_01056_D_20150413T184423_R13080_001.h5\", gdal.GA_ReadOnly)\n",
    "print(hdf_ds.GetSubDatasets())\n",
    "hdf_ds.GetMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From: https://earthdatascience.org/tutorials/create-raster-from-smap-soil-moisture-data/\n",
    "\n",
    "which_group = \"Geophysical_Data\"\n",
    "datasets = ['sm_rootzone_wetness']\n",
    "file_path = \"SMAP_L4_SM_gph_20170806T013000_Vv3030_001.h5\"\n",
    "\n",
    "def smap2raster(inputFile, group, dataset):\n",
    "    \"\"\"Converts SMAP data to a Raster object\n",
    "Input:  \n",
    "    inputFile - SMAP data file\n",
    "    group - The groupt containing the dataset we want to pull data from\n",
    "    dataset - Which specific dataset we want to pull data from\n",
    "Output: \n",
    "    A raster image in .tif format, saved to the current working directory\n",
    "    \"\"\"\n",
    "    #Read in the SMAP file in h5 format\n",
    "    h5File = h5py.File(inputFile, 'r')\n",
    "    \n",
    "    #Get the data from the specific group/dataset\n",
    "    data = h5File.get(group + '/' + dataset)\n",
    "    lat = h5File.get('cell_lat')\n",
    "    lon = h5File.get('cell_lon')\n",
    "    \n",
    "    #Convert this data into numpy arrays\n",
    "    np_data = np.array(data)\n",
    "    np_lat = np.array(lat)\n",
    "    np_lon = np.array(lon)\n",
    "    \n",
    "    #Get the spatial extents of the data\n",
    "    num_cols = float(np_data.shape[1])\n",
    "    num_rows = float(np_data.shape[0])\n",
    "    xmin = np_lon.min()\n",
    "    xmax = np_lon.max()\n",
    "    ymin = np_lat.min()\n",
    "    ymax = np_lat.max()\n",
    "    xres = (xmax - xmin)/num_cols\n",
    "    yres = (ymax - ymin)/num_rows\n",
    "    \n",
    "    #Set up the transformation necessary to create the raster\n",
    "    geotransform = (xmin, xres, 0, ymax, 0, -yres)\n",
    "    \n",
    "    #Create the raster object with the proper coordinate encoding and geographic transformation\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    raster = driver.Create(dataset+'Raster.tif', int(num_cols), int(num_rows), 1, gdal.GDT_Float32)\n",
    "    raster.SetGeoTransform(geotransform)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)\n",
    "    \n",
    "    #Export and write the data array to the raster\n",
    "    raster.SetProjection( srs.ExportToWkt() )\n",
    "    raster.GetRasterBand(1).WriteArray(np_data)\n",
    "    h5File.close()\n",
    "\n",
    "#Create an array of the datasets we want to use\n",
    "#Replace 'snow_mass' and 'snow_depth' with the datasets you want to use\n",
    "# datasets = ['sm_surface_wetness', 'soil_temp_layer2']\n",
    "\n",
    "#Loop through the datasets and create individual rasters from them\n",
    "for i in range(0, len(datasets)):\n",
    "    smap2raster(file_path, which_group, datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = 'sm_rootzone_wetness'\n",
    "inputFile = \"SMAP_L4_SM_gph_20170806T013000_Vv3030_001.h5\"\n",
    "group = \"Geophysical_Data\"\n",
    "\n",
    "h5File = h5py.File(inputFile, 'r')\n",
    "#Get the data from the specific group/dataset\n",
    "data = h5File.get(group + '/' + dataset)\n",
    "np_data = np.array(data)\n",
    "print(np_data.shape)\n",
    "\n",
    "lat = h5File.get('cell_lat')\n",
    "lon = h5File.get('cell_lon')\n",
    "print(len(lat))\n",
    "print(len(lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'SMAP_L4_SM_gph_20170806T013000_Vv3030_001.h5'\n",
    "h5file = h5py.File(file_path, 'r') \n",
    "\n",
    "which_group = 'Geophysical_Data'\n",
    "group = h5file.get(which_group)\n",
    "datasets = np.array(group)\n",
    "print(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
